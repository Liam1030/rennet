{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading rennet modules\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# py2.7 compat\n",
    "from __future__ import division, print_function\n",
    "from six.moves import zip, range, zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "rennet_data_root = os.path.join(\"..\", \"..\", \"data\")  # path to the data directory, AKA $RENNET_DATA_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***\n",
    "\n",
    "> **NOTE:**\n",
    ">\n",
    "> This is not the original notebook used in double-talk detection research by me (Abdullah).\n",
    "> But it is a faithful and more easy to use copy with some modifications, and some parts skipped.\n",
    ">\n",
    "> You should still be able to use it to meet the main goals of this notebook.\n",
    ">\n",
    "> For anything marked as `[SKIPPED]`, please refer to the following original notebooks in `notebooks/dtfinale/`:\n",
    "> - `2017-02-09-fisher-fe_03_p1.ipynb`\n",
    "> - `2017-03-22-fisher-fe_03_p1m-analysis.ipynb`\n",
    "> - `2017-03-23-fisher-fe_03_p1m-export-wav8kmono.ipynb`\n",
    "> - `2017-03-31-fisher-fe_03_p1-export-wav8kmono.ipynb`\n",
    "\n",
    "---\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition and Analysis of raw dataset `fisher/fe_03_p1`\n",
    "\n",
    "The goal of this notebook is to document how:\n",
    "- the raw `fisher/fe_03_p1` dataset was acquired,\n",
    "- the dataset was copied into a structure `working` directory to be used for analysis,\n",
    "- the required classes and functions were implemented to analyze the dataset,\n",
    "- the analysis of the various relevant properties of the dataset was performed. `[SKIPPED]`\n",
    "- the dataset was split into training, validation and testing sub-sets.\n",
    "- the data was exported in at standardized format for the three splits\n",
    "    + exporting to standard format involved converting all audio files to ones with the following properties:\n",
    "        * format: `wav`\n",
    "        * channels: `mono`\n",
    "        * samplerate: `8000 Hz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "The corpus is the first half of a collection of conversational telephone speech (CTS) created at LDC during 2003.\n",
    "\n",
    "It contains **5850** audio files, each with one full conversation of **upto 10 minutes** between **2 participants**.\n",
    "\n",
    "**Origin Location on disk**\n",
    "\n",
    "- AUDIO\n",
    "    + `/nm-raid/audio/data/corpora/LDC/fisher_eng_tr_sp_LDC2004S13/fisher_eng_tr_sp_LDC2004S13.zip`\n",
    "- LABELS\n",
    "    + `/nm-raid/audio/data/corpora/LDC/Other/LDC2004T19.tgz`\n",
    "\n",
    "The audio files are NIST Sphere files (`.sph`), with two channels, one per speaker, `(0: A, 1: B)`. \n",
    "The files are grouped into directories of a 100 files each, while the groups are available on 7 different discs.\n",
    "\n",
    "The `filetable.txt` has complete listing of all the files in this part of the dataset, including the gender of the speakers.\n",
    "\n",
    "The labels come in two forms: One that was extracted in an automated way marking speech parts, in `data/bbn_orig/`.\n",
    "The relevant transcription is in `data/trans/`, which are `.txt` files in groups of 100 files as above.\n",
    "The labels are however not divided based on discs.\n",
    "The `doc` folder has useful readmes and metadata for the recordings, with more information about the conversation and the speakers involved.\n",
    "\n",
    "The raw files mentioned above were **manually** copied into `$RENNET_DATA_ROOT/raw/fisher/fe_03_p1/` to maintain a local canonical copy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying files to `working` folder\n",
    "\n",
    "The files were copied **manually, again**, to `$RENNET_DATA_ROOT/working/fisher/fe_03_p1/raw` with the following directory structure.\n",
    "\n",
    "- `audio`\n",
    "    + has the readme file, and the `filetable.txt` with list of all audio files, and corresponding speaker genders\n",
    "    + `data/disc1` to `data/disc7` with grouped audio sph files, each group having roughly a hundred of them.\n",
    "        * the groups are named based on the first 3 digits of the conversation IDs of the files in them.\n",
    "- `labels`\n",
    "    + has readmes and doc files with more info about the transcriptions and the metadata in the same folder\n",
    "        * `fe_03_p1_calldata.tbl` has most of the relevant speaker and annotation metadata\n",
    "        * `fe_03_pindata.tbl` has deeper information about the speaker themselves.\n",
    "        * `fe_03_topics.sgm` is an xml like file with the information about the topics of conversation, referred in the `calldata` file\n",
    "    + Same as above, `data/disc1` to `data/disc7` with grouped transcription txt files, each group having roughly a hundred of them.\n",
    "        * the groups are named based on the first 3 digits of the conversation IDs of the files in them.\n",
    "        \n",
    "---\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing useful classes and functions\n",
    "\n",
    "Useful classes and functions will be implemented in this stage to be able to read the data and labels, and, more specifically, be able to work with the audio and label files for training double-talk detection model later on.\n",
    "\n",
    "### Gather all filepaths\n",
    "\n",
    "> **NOTE:**\n",
    ">\n",
    "> If it is not clear by now, we will be, from now on, working exclusively with the working directory we created in the previous step. Keep that in mind for all the instructions to come.\n",
    "\n",
    "\n",
    "To gather:\n",
    "- Paths to all audio files (.sph)\n",
    "- Paths to all transcriptions to be used as labels (.txt)\n",
    "- Path to the calldata\n",
    "    * There is more information available in other files, but the one in this file is enough for double-talk detection\n",
    "    * Actually, even that is not necessary ... but ... maybe it will be useful later. It has speaker PINs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rennet.utils.py_utils import recursive_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-09T18:11:07.829741",
     "start_time": "2017-02-09T18:11:07.735875"
    }
   },
   "outputs": [],
   "source": [
    "# Finding audio files\n",
    "\n",
    "rennet_workingdir = os.path.join(rennet_data_root, 'working')\n",
    "provider = 'fisher'\n",
    "dataset = 'fe_03_p1'\n",
    "\n",
    "working_rawaudio_dir = os.path.join(rennet_workingdir, provider, dataset, \n",
    "                                    'raw', 'audio', 'data')\n",
    "\n",
    "glob_root = working_rawaudio_dir\n",
    "glob_pattern = \"*.sph\"\n",
    "\n",
    "\n",
    "print(\"Query:\\n\", \"Root:{}\\n\".format(glob_root), \"Pattern:{}\\n\".format(glob_pattern))\n",
    "audio_fp = sorted(list(recursive_glob(glob_root, glob_pattern)))\n",
    "\n",
    "print(\"Found audio files: {}\".format(len(audio_fp)))\n",
    "print(\"\\n\".join(audio_fp[:10]), \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-09T18:12:58.694303",
     "start_time": "2017-02-09T18:12:58.599689"
    }
   },
   "outputs": [],
   "source": [
    "# Finding label files\n",
    "\n",
    "working_rawlabel_dir = os.path.join(rennet_workingdir, provider, dataset, \n",
    "                                    'raw', 'labels', 'data')\n",
    "\n",
    "glob_root = working_rawlabel_dir\n",
    "glob_pattern = \"*.txt\"\n",
    "\n",
    "\n",
    "print(\"Query:\\n\", \"Root: {}\\n\".format(glob_root), \"Pattern: {}\\n\".format(glob_pattern))\n",
    "label_fp = sorted(list(recursive_glob(glob_root, glob_pattern)))\n",
    "\n",
    "print(\"Found transcription files: {}\".format(len(label_fp)))\n",
    "print(\"\\n\".join(label_fp[:10]), \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the calldata file\n",
    "\n",
    "glob_root = os.path.join(rennet_workingdir, provider, dataset, \n",
    "                         'raw', 'labels')\n",
    "glob_pattern = \"*calldata.tbl\"\n",
    "\n",
    "print(\"Query:\\n\", \"Root: {}\\n\".format(glob_root), \"Pattern: {}\\n\".format(glob_pattern))\n",
    "calldata_fp = sorted(list(recursive_glob(glob_root, glob_pattern)))\n",
    "\n",
    "print(\"Found calldata files: {}\".format(len(calldata_fp)))\n",
    "print(\"\\n\".join(calldata_fp), \"\\n\")\n",
    "calldata_fp = calldata_fp[0]\n",
    "print(\"Choosing FIRST ONE:\\n{}\".format(calldata_fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### Make sure all audio files have corresponding labels, and vice-versa\n",
    "\n",
    "We are going to do this by comparing the `CALLID` in the filename of the audios and labels (transciptions).\n",
    "\n",
    "For this, a new module has been created as `rennet/rennet/datasets/fisher.py` which will house all the necessary classes and functions to be used for working with `fisher/fe_03_p1` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the created datasets.fisher module with support for autoreload\n",
    "# messing with complicated classes will require restarting this notebook's kernel ... but .. oh well\n",
    "%aimport rennet.datasets.fisher\n",
    "import rennet.datasets.fisher as fe\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if each pair has the same callid\n",
    "audio_callids = list(map(fe.callid_for_filename, audio_fp))\n",
    "label_callids = list(map(fe.callid_for_filename, label_fp))\n",
    "\n",
    "if len(audio_callids) != len(label_callids):\n",
    "    warnings.warn(\"\\nMismatch in number of audios ({}) vs. labels ({})\".format(len(audio_callids), len(label_callids)))\n",
    "else:\n",
    "    # same number of audios and labels\n",
    "    pass\n",
    "    \n",
    "# NOTE: it is assumed that audio_fp and label_fp are sorted, and hence are their callids\n",
    "if any(ac != lc for ac, lc in zip(audio_callids, label_callids)):\n",
    "    warnings.warn(\"\\nMismatch in callids for certain files\")\n",
    "else:\n",
    "    # even if there is a mismatch in lengths of lists of audios and labels\n",
    "    # all callids in the smallest list have a corresponding matching callid in the the other, at the same index\n",
    "    pass\n",
    "    \n",
    "    \n",
    "# No UserWarning means all okay, hopefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### How to read `.sph` audio files?\n",
    "\n",
    "A method was implemented in `rennet.utils.pydub_utils.AudioIO` class to read `.sph` files.\n",
    "It uses the `sph2pipe` tool.\n",
    "\n",
    "We will choose one audio file and try reading it, then export it to `.wav`.\n",
    "\n",
    "The as is exported file will have the two speaker channels separated.\n",
    "There is also code below on how to merge the two channels and export the mono-channel file.\n",
    "Once an audio has been read into an `AudioIO` object, (hopefully) any operations applicable on `pydub.AudioSegment` objects are available. (Google `pydub` for more info).\n",
    "\n",
    "\n",
    "We won't be converting all the files to `.wav` right now. \n",
    "That will be done while creating the training, validation and testing splits later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport rennet.utils.pydub_utils\n",
    "import rennet.utils.pydub_utils as bu\n",
    "\n",
    "%aimport rennet.utils.audio_utils\n",
    "import rennet.utils.audio_utils as au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if we can read .sph files and convert them to .wav files\n",
    "audio_0 = audio_fp[0]\n",
    "\n",
    "# Uncomment below and change after sph2pipe has been compiled. \n",
    "# Run as is to get instructions from the error on how.\n",
    "#\n",
    "# sph2pipe_path = os.path.join(rennet_root, 'rennet', 'utils', 'sph2pipe_v2.5', 'sph2pipe')\n",
    "# print(\"Path to compiled sph2pipe binary to be used: {}\".format(sph2pipe_path))\n",
    "\n",
    "# audio_0_io = bu.AudioIO.from_file(audio_0, sph2pipe_path=sph2pipe_path)\n",
    "audio_0_io = bu.AudioIO.from_file(audio_0)  # comment this out when you have sph2pipe. Use the line above.\n",
    "\n",
    "# convert to .wav without any changes\n",
    "to_fp = os.path.abspath(os.path.join(\".\", os.path.basename(audio_0) + \".wav\"))\n",
    "audio_0_io.export(out_f=to_fp, format='wav')\n",
    "wav_meta = au.get_audio_metadata(to_fp)\n",
    "print(\"Exported wav file can be found at:\\n{}\".format(wav_meta.filepath))\n",
    "print(\"\\n\", wav_meta, \"\\n\")\n",
    "\n",
    "# convert to mono-channel .wav\n",
    "audio_0_io_mono = audio_0_io.set_channels(1)\n",
    "to_fp = os.path.abspath(os.path.join(\".\", os.path.basename(audio_0) + \".mono.wav\"))\n",
    "audio_0_io_mono.export(out_f=to_fp, format='wav')\n",
    "wav_meta = au.get_audio_metadata(to_fp)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Exported mono wav file can be found at:\\n{}\".format(wav_meta.filepath))\n",
    "print(\"\\n\", wav_meta, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Reading the label (transcription) files\n",
    "\n",
    "The class `rennet.datasets.fisher.Annotations` was implemented to read transcriptions into the `SequenceLabels` structure.\n",
    "\n",
    "`Annotations.from_file(...)` was implemented using `csv` module to instantiate the class.\n",
    "\n",
    "**NOTE:** It is assumed that there are only 2 speakers in every file, one per channel, with `A` representing the first speaker, and the `B` for the second speaker, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how does the transcription file look like?\n",
    "label_0 = label_fp[0]\n",
    "\n",
    "with open(label_0, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of Annotations for the file\n",
    "ann = fe.Annotations.from_file(label_0)\n",
    "\n",
    "print(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all files can be read this way\n",
    "for l in label_fp:\n",
    "    fe.Annotations.from_file(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Reading extra call information from the calldata file\n",
    "\n",
    "The transcription file doesn't have more information about the speakers or the conversation.\n",
    "These potentially useful informations are available in the calldata file found earlier.\n",
    "\n",
    "`rennet.datasets.fisher.AllCallData` was impelemented as a slots-only class that parses **all** the calldata from the given path to calldata table file found earlier.\n",
    "\n",
    "It has more convenience methods, an interesting one is to get the calldata for a `CALLID` or even the filename by using the __getitem__ operator.\n",
    "\n",
    "Nevertheless, the `Annotations.from_file(...)` method now also accepts an `AllCallData` object (or filepath to it) and automatically parses the and make a copy of the relevant `CallData` from it. \n",
    "`CallData.channelspeakers` is a list of `Speaker` objects, with 2 speakers, for channel `A` and channel `B`, in order.\n",
    "\n",
    "For more information about what the parsed `CallData` means, refer to `doc_calldata_tbl.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcalldata = fe.AllCallData.from_file(calldata_fp)\n",
    "print(allcalldata)\n",
    "print()\n",
    "callid_0 = label_callids[0]\n",
    "print(callid_0)\n",
    "print(allcalldata[callid_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Annotations with calldata\n",
    "ann = fe.Annotations.from_file(label_0, allcalldata=allcalldata)\n",
    "print()\n",
    "print(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ann.calldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Inferring overlapping speech instances from the transcriptions\n",
    "\n",
    "It was seen that the transcriptions only consist of annotations for which speaker-channel is active during which intervals, and what is being spoken. \n",
    "Silences (or non-speech) events are implicitly annotated for by there being no entry for any speaker within that duration. \n",
    "\n",
    "This has been more or less faithfully been parsed into the `Annotations` object above. `ann.start_ends` has the start- and end-time-stamps for a line in the file, and `ann.labels` has the corresponding parsed transcription.\n",
    "\n",
    "`rennet.datasets.fisher.ActiveSpeakers` was implemented to parse from the `Annotation` object which speaker is active at _**all**_ the intervals between the first and the last entry (when sorted based on start and end time-stamps). \n",
    "\n",
    "This was done by _**assuming**_ that an absent `Transcription` (because of an absent entry for a speaker) indicates silence. \n",
    "This is a dangerous assumption since there can be other non-speech or even speech events missing from the transcription.\n",
    "However, nothing can be done for missing entries for durations between the first and last entries, and both speakers are assumed inactive (represented by `[0 0]` entry in the labels below).\n",
    "But, it is nevertheless, un-safe to assume this beyond the first and last entries, and so is not done in `ActiveSpeakers`, and should be kept in mind later when extracting labels for feature vectors at time-stamps outside this range.\n",
    "\n",
    "`ActiveSpeakers`'s parent class's `labels_at` method can be used to find which speaker is active at a given time-stamp. And from that, it can be determined whether the label for that time-stamp would be non-speech, single-speech, or overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract ActiveSpeakers from Annotations\n",
    "act = fe.ActiveSpeakers.from_annotations(ann)\n",
    "print(act)\n",
    "\n",
    "# [1, 0] : only speaker in channel A active (single-speech)\n",
    "# [0, 1] : only speaker in channel B active (single-speech)\n",
    "# [1, 1] : both speakers active (overlap)\n",
    "# [0, 0] : no speaker active (silence, or non-speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActiveSpeakers directly from filepath\n",
    "act = fe.ActiveSpeakers.from_file(label_0, allcalldata=allcalldata)\n",
    "print(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first entry's start time\n",
    "print(act.min_start)\n",
    "\n",
    "# last entry's end time\n",
    "print(act.max_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting active-speakers for time-stamps\n",
    "\n",
    "ts = [0, 375, 376, 1350, 1359, 1360, 1390, 1396, 1397, 1400, 50734, 50735]  # milliseconds\n",
    "ts_samplerate = 100  # milliseconds\n",
    "\n",
    "ts_actspk = act.labels_at(ends=ts, samplerate=ts_samplerate, \n",
    "                          default_label='zeros')  # assume no speaker active at ts outside (default behavior)\n",
    "\n",
    "print(\"\\n\".join(str(z) for z in zip(ts, ts_actspk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise error (not the default) if query ts are outside the first and last annotations\n",
    "ts_actspk = act.labels_at(ends=ts, samplerate=ts_samplerate, \n",
    "                          default_label='raise')  # raise error if any of ts is beyond first and last annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting labels at time-stamps\n",
    "ts_actspk = act.labels_at(ends=ts, samplerate=ts_samplerate)  # assume no speaker active at ts outside (default behavior)\n",
    "\n",
    "ts_labels = ts_actspk.sum(axis=1)  # labels_at(...) returns a numpy array when possible, esp. for default behavior\n",
    "\n",
    "ts_labels.clip(0, 2)  # not applicable here since there are only two speakers, but whatever\n",
    "\n",
    "labels_dict = {\n",
    "    0: 'non-speech',\n",
    "    1: 'single-speech',\n",
    "    2: 'overlap',\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\n\".join(str((t, labels_dict[l])) for t, l in zip(ts, ts_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all files can be read this way\n",
    "for l in label_fp:\n",
    "    fe.ActiveSpeakers.from_file(l, warn_duplicates=False)  # suppress the warnings for duplicates. We can't do much about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis `[SKIPPED]`\n",
    "\n",
    "> Here, various analyses would have been performed on the acquired, and now parsed, data.\n",
    "> For example, what is the typical duration of overlaps, etc.\n",
    "> These analyses influenced some decisions in the next steps below and in the subsequent notebooks.\n",
    ">\n",
    "> This section has been skipped from this notebook.\n",
    "> \n",
    "> Please refer to the original notebooks listed near the top of this notebook for inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assigning callids to training, validation and testing splits\n",
    "\n",
    "All audio and label files have been grouped based on the first 3 digits of their respective `CALLID`.\n",
    "These 3 digits will be called the `GROUPID` from here on forwards.\n",
    "A convenience function to do this was implemented in `rennet.datasets.fisher`\n",
    "\n",
    "> **NOTE:**\n",
    "> \n",
    "> The data-analysis that lead to the following decisions have been skipped from this notebook.\n",
    "> As noted previously for concrete analyses, please refer to the orginal notebooks.\n",
    "> The decisions, however, have been documented and implemented below.\n",
    "\n",
    "The idea now is that, there is so much data, that making splits based just on groups should be enough.\n",
    "\n",
    "- The variances across the groups are expected to be insignificant, wrt double-talk and it's contributing factors\n",
    "    + even 1% of the data is in actuality 9 hours long, roughly 10x what ka3 has.\n",
    "- It is very easy to assign groups, or even entire discs to a particular split.\n",
    "    + ***IDEALLY*** this whole thing should be randomized\n",
    "- Cross-validation on this dataset, even if arguably will be good, it is definitely impractical\n",
    "    + a 90-10 train-test split seems more than enough, given 10% will be roughly 90 hours\n",
    "    + Again, **Variances should be tested for** beforehand\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupid_for_filename = lambda fn: fe.groupid_for_callid(fe.callid_for_filename(fn))\n",
    "\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the audio and label filepaths and group them by groupid\n",
    "grouped_pair_fps = [(groupid, tuple(it)) \n",
    "                    for groupid, it in groupby(zip_longest(audio_fp, label_fp), \n",
    "                                                   lambda a_l: groupid_for_filename(a_l[1]))]\n",
    "\n",
    "print(\"Number of Groups: {}\\n\".format(len(grouped_pair_fps)))\n",
    "print(\"List of groupids:\\n{}\\n\".format([g for g, _ in grouped_pair_fps]))\n",
    "print(\"Number of files in each groupid:\\n{}\\n\".format([len(p) for _, p in grouped_pair_fps]))\n",
    "print(\"For Example: \")\n",
    "print(grouped_pair_fps[0][0])\n",
    "print(\"\\n\".join(str(pair) for pair in grouped_pair_fps[0][1][:3]), \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the per-file durations for these pairs, based on labels\n",
    "def duration_from_labelfp(fp):\n",
    "    ann = fe.Annotations.from_file(fp)\n",
    "    dur = ann.max_end - ann.min_start  # unit: (1/ann.samplerate) seconds \n",
    "    return dur / ann.samplerate  # unit: seconds\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "perfile_durs_from_labels = np.array(list(map(duration_from_labelfp, label_fp)))\n",
    "\n",
    "grouped_durs_from_labels = [(g, np.array([d for _, d in it])) \n",
    "                            for g, it in groupby(zip(label_fp, perfile_durs_from_labels), \n",
    "                                                 lambda l_p: groupid_for_filename(l_p[0]))]\n",
    "\n",
    "# NOTE: doing based on audios is possible, but not done here. \n",
    "# There will be disparities, but they were found to be negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_seconds(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return int(h), int(m), round(s, 2)\n",
    "\n",
    "def print_splitsec(duration, name):\n",
    "    print(\"{} : {:10.2f} seconds = {}\".format(\n",
    "        name, duration, \n",
    "        \"{:4} : {:2} : {:5.2f}\".format(*split_seconds(duration))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"durations statistics for files\")\n",
    "print_splitsec(np.min(perfile_durs_from_labels),   \"Minimum \")\n",
    "print_splitsec(np.max(perfile_durs_from_labels),   \"Maximum \")\n",
    "print_splitsec(np.mean(perfile_durs_from_labels),  \"Mean    \")\n",
    "print_splitsec(np.std(perfile_durs_from_labels),   \"Std     \")\n",
    "print_splitsec(np.sum(perfile_durs_from_labels),   \"Total   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"duration statistics for groups\")\n",
    "group_dur_from_labels = np.array([d.sum() for _, d in grouped_durs_from_labels])\n",
    "print_splitsec(np.min(group_dur_from_labels),   \"Minimum \")\n",
    "print_splitsec(np.max(group_dur_from_labels),   \"Maximum \")\n",
    "print_splitsec(np.mean(group_dur_from_labels),  \"Mean    \")\n",
    "print_splitsec(np.std(group_dur_from_labels),   \"Std     \")\n",
    "print_splitsec(np.sum(group_dur_from_labels),   \"Total   \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting decisions\n",
    "\n",
    "- Assigning the group `000` exclusively for validation.\n",
    "    + affords ~17 hours of data, ~1.7% of non-test data.\n",
    "    + It is still a lot of data to run validation after every-epoch.\n",
    "    + Therefore, some specific `callids` have been chosen, and are available at `rennet.datasets.fisher.chosen_val_callids` based on analysis on gender ratios, double-talk ratios, etc.\n",
    "        * We will still extract features for all validation calls, but will run our validations only on these calls during training.\n",
    "        * The entire validation set will be available for any post-training validation, or something.\n",
    "- Assigning all groups from `001` to `052` (inclusive) for training.\n",
    "    + This is a giant dataset.\n",
    "    + Definitely not all will be used for training, because of hard-disk limitation as is.\n",
    "        * Will be even possible to have completely disjoint set different training sub-splits!!!\n",
    "        * All while maintaining disjoint testing and validation splits.\n",
    "    + And, also, will probably only be unnecessary for training on only 3 classes.\n",
    "    + Assigning it now anyway, even though not even features will be extracted for all of them.\n",
    "        * Mainly so that there is a hard separation between all the splits.\n",
    "        * No calls from one split will be used for any other purpose.\n",
    "    + **The final training dataset will be formed using groups `001` to `012`** (1200 files)\n",
    "        * Based on ... how much the hard-disk could handle.\n",
    "        * It will still be ~189 hours of data.\n",
    "- Assigning all groups from `053` onwards for testing.\n",
    "    + All files from this assignment will be used during evaluation, and, hence, also feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fe.chosen_val_callids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign grouped pairs to splits\n",
    "val_groups = [grouped_pair_fps[0]]  # group `000`\n",
    "\n",
    "tis = 53\n",
    "trn_groups = grouped_pair_fps[1:tis]  # groups `001` to `052` (inclusive)\n",
    "\n",
    "tst_groups = grouped_pair_fps[tis:]  # groups `053` to `058` (inclusive)\n",
    "\n",
    "assert set(val_groups).isdisjoint(trn_groups)\n",
    "assert set(val_groups).isdisjoint(tst_groups)\n",
    "assert set(tst_groups).isdisjoint(trn_groups)\n",
    "\n",
    "# No Assertion errors means that all splits are disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Groups and Files per split:\\n\")\n",
    "\n",
    "trn_val_groups = trn_groups + val_groups\n",
    "print(\"    VAL with {:2} groups of total {:4} files\".format(len(val_groups), sum(map(len, (p for _, p in val_groups)))))\n",
    "print(\"    TRN with {:2} groups of total {:4} files\".format(len(trn_groups), sum(map(len, (p for _, p in trn_groups)))))\n",
    "print(\"    TST with {:2} groups of total {:4} files\".format(len(tst_groups), sum(map(len, (p for _, p in tst_groups)))))\n",
    "print(\"TRN+VAL with {:2} groups of total {:4} files\".format(len(trn_val_groups), sum(map(len, (p for _, p in trn_val_groups)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_durs_dict = {g: d.sum() for g, d in grouped_durs_from_labels}\n",
    "\n",
    "val_group_dur = sum(grouped_durs_dict[g] for g, _ in val_groups)\n",
    "trn_group_dur = sum(grouped_durs_dict[g] for g, _ in trn_groups)\n",
    "tst_group_dur = sum(grouped_durs_dict[g] for g, _ in tst_groups)\n",
    "trn_val_group_dur = val_group_dur + trn_group_dur\n",
    "tot_dur = trn_val_group_dur + tst_group_dur\n",
    "\n",
    "print(\"Precentages and Durations of Splits\")\n",
    "print_splitsec(val_group_dur,     \"    VAL  == {:6.2f}%\".format(100 *     val_group_dur/tot_dur))\n",
    "print_splitsec(trn_group_dur,     \"    TRN  == {:6.2f}%\".format(100 *     trn_group_dur/tot_dur))\n",
    "print_splitsec(tst_group_dur,     \"    TST  == {:6.2f}%\".format(100 *     tst_group_dur/tot_dur))\n",
    "print_splitsec(trn_val_group_dur, \"TRN+VAL  == {:6.2f}%\".format(100 * trn_val_group_dur/tot_dur))\n",
    "print_splitsec(tot_dur,           \"  TOTAL  == {:6.2f}%\".format(100 *           tot_dur/tot_dur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exporting splits to `wav-8k-mono` standard format\n",
    "\n",
    "\n",
    "\n",
    "### The folder structure expected for exports\n",
    "\n",
    "```\n",
    "$RENNET_DATA_ROOT/working/<provider>/<dataset>/<export_name>/\n",
    "    - test/\n",
    "        - audio/\n",
    "            - data/\n",
    "                - <group-number>/\n",
    "                    - <call>.wav\n",
    "                    - ...\n",
    "         - labels/\n",
    "             - data/\n",
    "                 - <group-number>/\n",
    "                     - <call>.txt\n",
    "                     - ...\n",
    "             - <calldata>.tbl\n",
    "    - train/\n",
    "        - < SAME AS TEST >\n",
    "    - val/\n",
    "        - < SAME AS TEST >\n",
    "    - pickles/\n",
    "        - <date-stamped-features_1-info>/\n",
    "            - trn.h5\n",
    "            - tst.h5\n",
    "            - val.h5\n",
    "        - <date-stamped-features_2-info>/\n",
    "            - trn.h5\n",
    "            - tst.h5\n",
    "            - val.h5\n",
    "        - ...\n",
    "```\n",
    "\n",
    "> **NOTE:**\n",
    ">\n",
    "> The directories inside `pickles` above will be created in the feature-extraction notebook; shown here for reference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE HERE - BEGIN #######################################################\n",
    "\n",
    "export_name = 'wav-8k-mono'\n",
    "val_dirname = 'val'\n",
    "trn_dirname = 'train'\n",
    "tst_dirname = 'test'\n",
    "\n",
    "out_format = 'wav'\n",
    "out_samplerate = 8000\n",
    "out_channels = 1\n",
    "out_channels_split = False\n",
    "\n",
    "# UPDATE HERE - END #########################################################\n",
    "rennet_working_dir = rennet_workingdir\n",
    "\n",
    "export_name_dir = os.path.join(rennet_working_dir, provider, dataset, export_name)\n",
    "val_dir = os.path.join(export_name_dir, val_dirname)\n",
    "trn_dir = os.path.join(export_name_dir, trn_dirname)\n",
    "tst_dir = os.path.join(export_name_dir, tst_dirname)\n",
    "\n",
    "val_labeldata_dir = os.path.join(val_dir, 'labels', 'data')\n",
    "trn_labeldata_dir = os.path.join(trn_dir, 'labels', 'data')\n",
    "tst_labeldata_dir = os.path.join(tst_dir, 'labels', 'data')\n",
    "\n",
    "val_audiodata_dir = os.path.join(val_dir, 'audios', 'data')\n",
    "trn_audiodata_dir = os.path.join(trn_dir, 'audios', 'data')\n",
    "tst_audiodata_dir = os.path.join(tst_dir, 'audios', 'data')\n",
    "\n",
    "pickles_dir = os.path.join(export_name_dir, 'pickles')\n",
    "\n",
    "print(\"Directories that will be created, upto group-name (exclusive)\\n\")\n",
    "print(\"VAL:\", val_dir, val_labeldata_dir, val_audiodata_dir, sep='\\n')\n",
    "print()\n",
    "print(\"TRN:\", trn_dir, trn_labeldata_dir, trn_audiodata_dir, sep='\\n')\n",
    "print()\n",
    "print(\"TST:\", tst_dir, tst_labeldata_dir, tst_audiodata_dir, sep='\\n')\n",
    "print()\n",
    "print(\"PICKLES:\", pickles_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the data directories for the splits\n",
    "from rennet.utils.py_utils import makedirs_with_existok\n",
    "\n",
    "# these directories shouldn't exist right now\n",
    "\n",
    "# ##### DO NOT RERUN! without changing ddeo to True ######\n",
    "ddeo = False\n",
    "\n",
    "makedirs_with_existok(val_labeldata_dir, exist_ok=ddeo)\n",
    "makedirs_with_existok(val_audiodata_dir, exist_ok=ddeo)\n",
    "makedirs_with_existok(trn_labeldata_dir, exist_ok=ddeo)\n",
    "makedirs_with_existok(trn_audiodata_dir, exist_ok=ddeo)\n",
    "makedirs_with_existok(tst_labeldata_dir, exist_ok=ddeo)\n",
    "makedirs_with_existok(tst_audiodata_dir, exist_ok=ddeo)\n",
    "makedirs_with_existok(pickles_dir, exist_ok=ddeo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group-name dir paths\n",
    "groups_for_split = lambda split: [g for g, _ in split]\n",
    "val_groupdirnames = groups_for_split(val_groups)\n",
    "trn_groupdirnames = groups_for_split(trn_groups)\n",
    "tst_groupdirnames = groups_for_split(tst_groups)\n",
    "\n",
    "val_audiogroup_dirs = [os.path.join(val_audiodata_dir, g) for g in val_groupdirnames]\n",
    "val_labelgroup_dirs = [os.path.join(val_labeldata_dir, g) for g in val_groupdirnames]\n",
    "trn_audiogroup_dirs = [os.path.join(trn_audiodata_dir, g) for g in trn_groupdirnames]\n",
    "trn_labelgroup_dirs = [os.path.join(trn_labeldata_dir, g) for g in trn_groupdirnames]\n",
    "tst_audiogroup_dirs = [os.path.join(tst_audiodata_dir, g) for g in tst_groupdirnames]\n",
    "tst_labelgroup_dirs = [os.path.join(tst_labeldata_dir, g) for g in tst_groupdirnames]\n",
    "\n",
    "print(\"VAL:\", \"\\n\".join(val_audiogroup_dirs), '', \"\\n\".join(val_labelgroup_dirs))\n",
    "print()\n",
    "print(\"TRN:\", \"\\n\".join(trn_audiogroup_dirs), '', \"\\n\".join(trn_labelgroup_dirs))\n",
    "print()\n",
    "print(\"TST:\", \"\\n\".join(tst_audiogroup_dirs), '', \"\\n\".join(tst_labelgroup_dirs))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make group-name directories for all splits\n",
    "exist_ok = False\n",
    "for agd, lgd in zip(val_audiogroup_dirs, val_labelgroup_dirs):\n",
    "    makedirs_with_existok(agd, exist_ok=exist_ok)\n",
    "    makedirs_with_existok(lgd, exist_ok=exist_ok)\n",
    "    \n",
    "for agd, lgd in zip(trn_audiogroup_dirs, trn_labelgroup_dirs):\n",
    "    makedirs_with_existok(agd, exist_ok=exist_ok)\n",
    "    makedirs_with_existok(lgd, exist_ok=exist_ok)\n",
    "    \n",
    "for agd, lgd in zip(tst_audiogroup_dirs, tst_labelgroup_dirs):\n",
    "    makedirs_with_existok(agd, exist_ok=exist_ok)\n",
    "    makedirs_with_existok(lgd, exist_ok=exist_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to which calldata tbl for each split's label dir will be copied to\n",
    "# it is the same file, copying for consistency\n",
    "val_labeldir = os.path.dirname(val_labeldata_dir)\n",
    "trn_labeldir = os.path.dirname(trn_labeldata_dir)\n",
    "tst_labeldir = os.path.dirname(tst_labeldata_dir)\n",
    "\n",
    "print(\"labels dirs where calldata will be copied to:\", val_labeldir, trn_labeldir, tst_labeldir, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the calldata tbl file\n",
    "import shutil as sh\n",
    "\n",
    "print(sh.copy(calldata_fp, val_labeldir))\n",
    "print(sh.copy(calldata_fp, trn_labeldir))\n",
    "print(sh.copy(calldata_fp, tst_labeldir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy all label files to appropriate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_labelfile_split(labelfile_splitlabeldatadir):\n",
    "    labelfile, split_labeldatadir = labelfile_splitlabeldatadir\n",
    "    if labelfile is None:\n",
    "        return \"\"\n",
    "    \n",
    "    groupname = groupid_for_filename(labelfile)\n",
    "    \n",
    "    todir = os.path.join(split_labeldatadir, groupname)\n",
    "    sh.copy(labelfile, todir) \n",
    "    \n",
    "    return os.path.basename(labelfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mp.Pool(mp.cpu_count())\n",
    "\n",
    "splits_pbar = tqdm(zip([val_groups, trn_groups, tst_groups],\n",
    "                       [val_labeldata_dir, trn_labeldata_dir, tst_labeldata_dir],\n",
    "                       ['val', 'trn', 'tst']))\n",
    "\n",
    "for split, splitlabeldatadir, name in splits_pbar:\n",
    "    params = []\n",
    "    for _, pair in split:\n",
    "        for _, lfp in pair:\n",
    "            params.append((lfp, splitlabeldatadir))\n",
    "            \n",
    "    files_pbar = tqdm(total=len(params))\n",
    "    for fn in p.imap_unordered(copy_labelfile_split, params):\n",
    "        files_pbar.update()\n",
    "        files_pbar.set_description(fn)\n",
    "    splits_pbar.set_description(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = None\n",
    "files_pbar.close()\n",
    "splits_pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export all audio files to the approriate format, to the appropriate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_audiofile_split(audiofile_splitaudiodatadir_tofmt_tosr_tonchannels_issplit):\n",
    "    audiofile, splitaudiodatadir = audiofile_splitaudiodatadir_tofmt_tosr_tonchannels_issplit[:2]\n",
    "    if audiofile is None:\n",
    "        return \"\"\n",
    "    \n",
    "    tofmt, tosr, tonchannels, issplit = audiofile_splitaudiodatadir_tofmt_tosr_tonchannels_issplit[2:]\n",
    "    \n",
    "    groupname = os.path.basename(os.path.dirname(audiofile))\n",
    "    todir = os.path.join(splitaudiodatadir, groupname)\n",
    "    \n",
    "    if issplit:\n",
    "        tofns = bu.convert_to_standard_split(audiofile, todir, tofmt, tosr)\n",
    "    else:\n",
    "        tofns = bu.convert_to_standard(audiofile, todir, tofmt, tosr, tonchannels)\n",
    "        \n",
    "    return os.path.basename(tofns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mp.Pool(mp.cpu_count())\n",
    "\n",
    "splits_pbar = tqdm(zip([val_groups, trn_groups, tst_groups],\n",
    "                       [val_audiodata_dir, trn_audiodata_dir, tst_audiodata_dir],\n",
    "                       ['val', 'trn', 'tst']))\n",
    "\n",
    "for split, splitaudiodatadir, name in splits_pbar:\n",
    "    params = []\n",
    "    for _, pairs in split:\n",
    "        for afp, _ in pairs:\n",
    "            if afp is None:\n",
    "                continue\n",
    "            params.append((afp, splitaudiodatadir, \n",
    "                          out_format, out_samplerate, \n",
    "                          out_channels, out_channels_split))\n",
    "            \n",
    "    files_pbar = tqdm(total=len(params))\n",
    "    for fn in p.imap_unordered(export_audiofile_split, params):\n",
    "        files_pbar.update()\n",
    "        files_pbar.set_description(fn)\n",
    "    files_pbar.close()\n",
    "    splits_pbar.set_description(name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = None\n",
    "splits_pbar.close()\n",
    "files_pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check all files have been moved to the appropriate paths `[SKIPPED]`\n",
    "\n",
    "> **NOTE:**\n",
    ">\n",
    "> Sorry, you'll have to check the files manually.\n",
    ">\n",
    "> They should be fine, but some sanity check would be much better.\n",
    ">\n",
    "> Scripted checks are performed at the end of the notebook:\n",
    "> \n",
    "> `notebooks/dtfinale/2017-03-31-fisher-fe_03_p1-export-wav8kmono.ipynb`\n",
    ">\n",
    "> You can adapt that code, if you smell something fishy."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
