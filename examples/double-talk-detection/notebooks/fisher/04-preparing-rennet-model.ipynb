{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading rennet modules\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# py2.7 compat\n",
    "from __future__ import division, print_function\n",
    "from six.moves import zip, range, zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "rennet_data_root = os.path.join(\"..\", \"..\", \"data\")\n",
    "rennet_x_root = os.path.join(\"..\", \"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing `rennet_model` from trained `keras_model`\n",
    "\n",
    "The goal of this notebook is to document how:\n",
    "- the trained models were analysed `[SKIPPED]`\n",
    "- two trained models were chosen to created a combined `rennet_model`\n",
    "- the `rennet_model` reading and application class was implemented\n",
    "- the `rennet_model` was tuned to perform on some data\n",
    "- the `rennet_model` was exported to `model.h5` to be used by `annonet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing trained `keras_model`\n",
    "\n",
    "The analysis was done of all the `keras_model` checkpoints saved for all the training configs during the training phase.\n",
    "\n",
    "One `keras_model` checkpoint per config, and was __manually copied__ to a new file `model.h5` in the same activity output directory as the config.\n",
    "- For Abdullah's research, __the last__ `keras_model` checkpoint (for the last epoch of the last pass) of each config was chosen as the `model.h5`.\n",
    "- This was done because the training had stabilized, and there weren't many significant differences between the models' performance after the training had stabilized.\n",
    "- The evaluations of each config were performed on the entire testing split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Combine `keras_model`s for `m-n/keepzero` and `m-n/skipzero`\n",
    "\n",
    "- It was observed that __mean-normalization__ gave, overall, the best results for detecting double-talks, irrespective of the sub-sampling applied.\n",
    "- It was observed that `m-n/keepzero` config detected all three classes, very good with silence, very precise with double-talks (but with very low recall).\n",
    "- It was observed that `m-n/skipzero-20one` detected a lot more double-talks, but at loss of precision, and, as expected, without ever predicting for silence.\n",
    "\n",
    "Based on these observations, the decision was made to combine the best models from the two configs below in parallel to form the final `rennet_model`:\n",
    "1. `m-n/keepzero`\n",
    "2. `m-n/skipzero-20one`\n",
    "\n",
    "The combined model will take two inputs, and produce to softmax-predictions for the respectively ordered models.\n",
    "The inputs to both the models are the same, hence the feature extraction will be the same for both the inputs.\n",
    "\n",
    "The two ouputs of the model will be __merged (and normalized)__ based on some strategy to get new _faux_-softmax posteriors. \n",
    "Viterbi smoothing will then be applied to produce the final predictions.\n",
    "\n",
    "It was decided to first combine and export the two chosen `keras_model`s, and implement an ___exclusive___ `rennet_model` class in `rennet.models` which will be responsible for reading this combined model and applying it to a given speech file.\n",
    "\n",
    "The class includes the decision made on how to merge the predictions from the two models before viterbi smoothing.\n",
    "\n",
    "The merging strategy used is that of taking a weighted average of the two predictions, the weights being different for each of the three classes. Other strategies were investigated, but this one gave the most consistent result.\n",
    "\n",
    "The merging weights, in addition to the window size used for normalization (all parameters of the `rennet_model` are initialized in it's `__init__` method) are tunable.\n",
    "They have been set to hard-coded values based on which perform best on KA3 (client) data overall.\n",
    "\n",
    "Most of the ther hard-coded parameters are specific and exclusive to this particular combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5py import File as hf\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the models to merge\n",
    "\n",
    "outputs = os.path.join(rennet_x_root, \"outputs\", \"fisher-sample\")\n",
    "keepzero = os.path.join(outputs, \"m-n/keepzero/model.h5\")  # chosen model\n",
    "skipzero = os.path.join(outputs, \"m-n/skipzero/model.h5\")  # chosen model\n",
    "\n",
    "assert os.path.exists(keepzero)\n",
    "assert os.path.exists(skipzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chosen models\n",
    "m1 = load_model(keepzero)\n",
    "m2 = load_model(skipzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport rennet.utils.keras_utils\n",
    "import rennet.utils.keras_utils as ku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the models in parallel - ORDER MATTERS!!\n",
    "mm = ku.combine_keras_models_parallel([m1, m2])\n",
    "mm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined the combined model\n",
    "mm.compile(\n",
    "    optimizer='adamax',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the combined `keras_model`\n",
    "model_export_path = os.path.abspath(os.path.join(rennet_data_root, \"models\", \"model.h5\"))\n",
    "\n",
    "mm.save(model_export_path, overwrite=False)\n",
    "\n",
    "# We will add stuff to this model to make it into a compatible `rennet_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the hdf5 file\n",
    "f = hf(model_export_path, 'a')\n",
    "\n",
    "print(list(f.keys()))  # what keras has exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport rennet.utils.py_utils\n",
    "import rennet.utils.py_utils as pu\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the raw Viterbi priors from the trn.h5 and val.h5\n",
    "pickles_root = os.path.join(rennet_data_root, \"working\", \"fisher\", \"fe_03_p1\", \"wav-8k-mono\", \"pickles\")\n",
    "\n",
    "pickles_dir = sorted(glob.glob(os.path.join(pickles_root, \"*\")))  # all featx pickles directories\n",
    "print(\"Pickles directories found:\\n\", \"\\n\".join(str(p) for p in pickles_dir), '\\n')\n",
    "\n",
    "\n",
    "# ATTENTION: make sure this is the same one on which the models chosen above were trained on\n",
    "\n",
    "pickles_dir = pickles_dir[-1]  # choose the latest one\n",
    "print(\"Pickles directory CHOSEN:\\n\", pickles_dir)\n",
    "\n",
    "val_h5 = os.path.join(pickles_dir, \"val.h5\")\n",
    "trn_h5 = os.path.join(pickles_dir, \"trn.h5\")\n",
    "\n",
    "with hf(val_h5, 'r') as f_a:\n",
    "    vinit = f_a[\"viterbi/init\"][()]\n",
    "    vtran = f_a[\"viterbi/tran\"][()]\n",
    "    vpriors = f_a[\"viterbi/priors\"][()]\n",
    "\n",
    "with hf(trn_h5, 'r') as f_a:\n",
    "    tinit = f_a[\"viterbi/init\"][()]\n",
    "    ttran = f_a[\"viterbi/tran\"][()]\n",
    "    tpriors = f_a[\"viterbi/priors\"][()]\n",
    "\n",
    "init = vinit + tinit\n",
    "tran = vtran + ttran#mgref. priors is undefined in the following line. assuming:\n",
    "priors = vpriors + tpriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the viterbi priors in the combined model's h5 at `model/viterbi/...`\n",
    "f.create_group('rennet/model')\n",
    "f.create_group('rennet/model/viterbi')\n",
    "\n",
    "f.create_dataset('rennet/model/viterbi/init', data=init)\n",
    "f.create_dataset('rennet/model/viterbi/tran', data=tran)\n",
    "f.create_dataset('rennet/model/viterbi/priors', data=priors)\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rennet import __version__ as v\n",
    "print(v)  # the current version of rennet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the source version and the minimum supported version of rennet for which\n",
    "# the model we are creating will be valid\n",
    "f['rennet'].attrs['version_src'] = v\n",
    "f['rennet'].attrs['version_min'] = \"0.1.0\"  # min-version when the rennet_model class was implemented\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport rennet.models\n",
    "import rennet.models as rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the rennet_model class which was implemented EXCLUSIVELY for this type of model\n",
    "\n",
    "rennet_model = rm.DT_2_nosub_0zero20one_mono_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = str(rennet_model.__name__)\n",
    "\n",
    "# ATTENTION: Make sure that the name of the unique class from `rennet/models.py` is the correct one for this model\n",
    "print(model_name)  # DT_2_nosub_0zero20one_mono_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add info about which model class `annonet` should use to read/apply this model\n",
    "# Make sure this is the right class. It should have already been implemented\n",
    "\n",
    "f['rennet/model'].attrs['name'] = model_name\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save everything and close\n",
    "f.close()\n",
    "\n",
    "# The model.h5 is now a `rennet_model`\n",
    "\n",
    "print(\"The rennet_model was exported at:\\n\", model_export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.create_dataset('rennet/model/norm_winsec', data=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT REMINDER\n",
    "\n",
    "The the model that has been exported __will not be__ tracked by git.\n",
    "\n",
    "Provide it to your respective user, either separately, or bundled in the zip of the package at the location (in the `rennet` package you are preparing) `data/models/model.h5`.\n",
    "\n",
    "That location will be looked in by `annonet.py` to get the `rennet_model` and, based on the additional information that we added above, especially `rennet/model[name]`, the appropriate model class will be used to read and apply the model.\n",
    "\n",
    "Make sure to test an independent copy of the package (without any environment variables set), especially by running `annonet.sh`, before sending to a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis `[REFERENCE]`\n",
    "\n",
    "> The analysis code in the rest of the notebook is here only for your reference.\n",
    "> \n",
    "> It is __not guaranteed to work as__, but I hope that you will be able to infer what's going on, and adapt it to your needs\n",
    ">\n",
    "> As a hint, this code was written was to analyze a KA3 file for which annotations were available, and the final exported elan file consists of both the true and predicted labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pympi as pm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-muted')\n",
    "\n",
    "from itertools import chain, repeat, groupby\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% reloadable imports of `rennet` modules\n",
    "import rennet.utils.audio_utils as au\n",
    "import rennet.utils.label_utils as lu\n",
    "import rennet.utils.np_utils as nu\n",
    "import rennet.datasets.ka3 as k3\n",
    "import rennet.utils.plotting_utils as pu\n",
    "import rennet.utils.keras_utils as ku\n",
    "from rennet import models as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% filepaths\n",
    "working_raw_ka3 = os.path.join(rennet_data_root, \"working\", \"ka3\", \"deutsch-01\", \"raw\")\n",
    "audiofp = os.path.join(working_raw_ka3, \"media/DEU_pear_Iuna.wav\")\n",
    "labelfp = os.path.join(working_raw_ka3, \"labels/DEU_pear_Iuna.xml\")\n",
    "\n",
    "assert os.path.exists(audiofp)\n",
    "assert os.path.exists(labelfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "modelfp = model_export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "model = m.DT_2_nosub_0zero20one_mono_mn(modelfp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "with hf(\"./outputs/DEU_pear_Iuna.h5\") as f:\n",
    "    sad = f['sad'][()]\n",
    "    dtd = f['dtd'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "d = model.preprocess(audiofp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "sad, dtd = model.predict(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "with hf(\"./outputs/DEU_pear_Iuna.h5\") as f:\n",
    "    f.create_dataset('sad', data=sad)\n",
    "    f.create_dataset('dtd', data=dtd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "label = k3.ActiveSpeakers.from_file(labelfp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "nsamples = model.loadaudio(audiofp).shape[0]  # pylint: disable=no-member\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "ends = lu.samples_for_labelsat(nsamples, model.hop_len, model.win_len)[10:-10]\n",
    "Y = label.labels_at(ends, model.samplerate).sum(axis=1)\n",
    "list(map(len, (sad, dtd, Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def p(true, pred, nclasses=3, onlydiag=True):\n",
    "    nu.print_prec_rec(\n",
    "        *nu.normalize_confusion_matrix(\n",
    "            nu.confusion_matrix(true, pred, nclasses=nclasses)),\n",
    "        onlydiag=onlydiag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "p(Y, sad.argmax(axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "p(Y, dtd.argmax(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# we can even change the mergepreds_fn to something else\n",
    "merged = model.mergepreds_fn([sad, dtd])\n",
    "merged.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "p(Y, merged.argmax(axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "vit = lambda pred: lu.viterbi_smoothing(pred, model.vinit, model.vtran)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "p(Y, vit(sad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "p(Y, vit(dtd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "p(Y, vit(merged))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "seq = lu.ContiguousSequenceLabels.from_dense_labels(\n",
    "    vit(merged),\n",
    "    keep=model.seq_keep,\n",
    "    min_start=model.seq_minstart,\n",
    "    samplerate=model.seq_samplerate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "tofile = \"./outputs/{}.out.rrcomp.eaf\".format(os.path.basename(audiofp))\n",
    "tofile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "eaf = seq.to_eaf(\n",
    "    to_filepath=tofile,\n",
    "    linked_media_filepath=audiofp,\n",
    "    annotinfo_fn=model.seq_annotinfo_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "label_c = deepcopy(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "label_c.labels = label_c.labels.sum(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "with label.samplerate_as(1000):\n",
    "    print(label[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "with label_c.samplerate_as(1000):\n",
    "    print(label_c[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "label_tiers = {\n",
    "    0: \"true_none\",\n",
    "    1: \"true_single\",\n",
    "    2: \"true_multiple\",\n",
    "}\n",
    "annotinfo_fn = lambda label: lu.EafAnnotationInfo(tier_name=label_tiers[label])\n",
    "label_c.to_eaf(\n",
    "    to_filepath=tofile,\n",
    "    eafobj=eaf,\n",
    "    annotinfo_fn=annotinfo_fn,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
